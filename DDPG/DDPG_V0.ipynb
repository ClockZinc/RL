{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm\n",
    "![DDPG Algorithm](pic/DDPG.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法核心思想\n",
    "1.对于连续控制问题不太可能离散使用DQN来解决，直接用确定策略网络（deterministic policy network）生成一个确定的梯度 <br>\n",
    "2.其可以看作以DQN为基础，actor-critic架构辅助更新的算法<br>\n",
    "3.与DQN类似，考虑到同时更新critic和actor将会导致bootstrapping现象导致高估（overestimation） critic网络，对actor和critic网络使用target-network方法，在TD更新critic网络时替代$r_t+\\gamma q_{t+1}$，此处还能用multi-step TD targets的方法进一步提高critic网络的更新参数。参见上式第12行<br>\n",
    "4.更新完critic网络后，再更新actor网络，公式为$$\\nabla_\\theta \\pi(s_t;\\theta)\\times meanOF(\\nabla_\\pi Q(s_t,\\pi(s_t;\\theta);\\omega))$$\n",
    "5.此处还能用experience reply，建立一个足够大的（1e5）的reply buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法提升\n",
    "1.TD3<br>\n",
    "1.1学习double DQN的方法，用两个critic网络对动作值进行评估，以解决（fix）高估问题<br>\n",
    "1.2延迟更新,actor网络的更新频率比critic网络慢<br>\n",
    "1.3“梯度截取，这是在知乎中有人提到的，但是[文章](https://zhuanlan.zhihu.com/p/553179826)中并没有体现出来<br>\n",
    "1.4目标策略平滑，首先在探索时加入噪声（并clip一下），在生成target时加入噪声\n",
    "$$y=r+\\gamma Q_{\\theta^{\\prime}}\\left(s^{\\prime}, \\pi_{\\omega^{\\prime}}\\left(s^{\\prime}\\right)+\\epsilon\\right)$$\n",
    "$$\\epsilon \\sim \\operatorname{clip}(\\mathcal{N}(0, \\sigma),-c, c)$$\n",
    "但是考虑到误差问题，再次clip，最终为\n",
    "$$y=r+\\gamma Q_{\\theta^{\\prime}}\\left(s^{\\prime}, \\operatorname{clip}\\left(\\pi_{\\omega^{\\prime}}\\left(s^{\\prime}\\right)+\\epsilon\\right.\\right., min action, max action \\left.)\\right), \\epsilon \\sim \\operatorname{clip}(\\mathcal{N}(0, \\sigma),-c, c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总流程\n",
    "首先初始化四个网络参数<br>\n",
    "<1>actor 网络参数$\\theta$<br>\n",
    "<2>actor_target_network参数$\\theta^{\\prime}\\gets \\theta$<br>\n",
    "<3>critic 网络参数$\\omega$<br>\n",
    "<4>critic_target_network 参数$\\omega^{\\prime}\\gets \\omega$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进入循环<br>\n",
    "观测状态s,并且执行动作a，a为clip（actor网络的值加上一个正态分布的噪声）<br>\n",
    "执行a，得到下一时刻的状态s',reward r，和完成与否的flag d<br>\n",
    "将上述的transition $(s,a,r,s')$ 存到replay buffer里<br>\n",
    "如果d=1，即本次探索结束了，就从头开始reset()<br>\n",
    "重复上述操作，做到充分探索，直到replay buffer满了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果replay buffer满了，就开始更新网络参数<br>\n",
    "在一定次数的循环中\n",
    "采取小批量更新，从经验池中随机选取一定批量的transition\n",
    "计算targets\n",
    "$$y\\left(r, s^{\\prime}, d\\right)=r+\\gamma(1-d) Q_{\\omega^{\\prime}}\\left(s^{\\prime}, \\pi_{\\theta^{\\prime}}\\left(s^{\\prime}\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上式中 $(1-d)$ 表示是否完成，如果没有完成肯定是要加入Q的值，如果没有就不用加<br>\n",
    "此外，很明显使用了target network来辅助计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用TD算法 gradient descent 更新一次critic网络的值<br>\n",
    "$$\\nabla_{\\omega}meanOF(\\nabla_\\pi Q(s_t,\\pi(s_t;\\theta);\\omega))$$\n",
    "\n",
    "用policy gradient descent 更新一次actor网络的值\n",
    "$$\\nabla_\\theta \\pi(s_t;\\theta)\\times meanOF(\\nabla_\\pi Q(s_t,\\pi(s_t;\\theta);\\omega))$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用soft update 更新target network的值<br>\n",
    "$$\\begin{gathered}\n",
    "\\omega^{\\prime} \\leftarrow \\rho \\omega^{\\prime}+(1-\\rho) \\omega \\\\\n",
    "\\theta^{\\prime} \\leftarrow \\rho \\theta^{\\prime}+(1-\\rho) \\theta\n",
    "\\end{gathered}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更新数轮后，再进行探索更新replay buffer，不断循环即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代码部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境\n",
    "tensorflow==2.3<br>\n",
    "gym=0.23.1<br>\n",
    "pandas=1.3.5<br>\n",
    "numpy=0.18.5<br>\n",
    "详见requirements.txt<br>\n",
    "``` dos\n",
    "pip list --format=freeze> requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import\n",
    "引入<br>\n",
    "os以实现通过--train和--test与--seed设置训练、测试和seed模式<br>\n",
    "tensorflow实现神经网络的搭建<br>\n",
    "time实现计时<br>\n",
    "numpy实现部分数学运算与自由选择<br>\n",
    "datetime实现对数据的管理<br>\n",
    "matplotlib.pylot实现绘图需要<br>\n",
    "gym引入环境<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\New_Program_Files\\anaconda_V5_3_1\\envs\\DDPG_V3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:546: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  class IteratorBase(collections.Iterator, trackable.Trackable,\n",
      "d:\\New_Program_Files\\anaconda_V5_3_1\\envs\\DDPG_V3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:106: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  class DatasetV2(collections.Iterable, tracking_base.Trackable,\n",
      "d:\\New_Program_Files\\anaconda_V5_3_1\\envs\\DDPG_V3\\lib\\site-packages\\tensorflow\\python\\autograph\\utils\\testing.py:21: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "d:\\New_Program_Files\\anaconda_V5_3_1\\envs\\DDPG_V3\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "d:\\New_Program_Files\\anaconda_V5_3_1\\envs\\DDPG_V3\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "d:\\New_Program_Files\\anaconda_V5_3_1\\envs\\DDPG_V3\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "d:\\New_Program_Files\\anaconda_V5_3_1\\envs\\DDPG_V3\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  if hasattr(pil_image, 'HAMMING'):\n",
      "d:\\New_Program_Files\\anaconda_V5_3_1\\envs\\DDPG_V3\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  if hasattr(pil_image, 'BOX'):\n",
      "d:\\New_Program_Files\\anaconda_V5_3_1\\envs\\DDPG_V3\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  if hasattr(pil_image, 'LANCZOS'):\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 _*_\n",
    "# 系统级的函数\n",
    "import os,time,datetime,argparse\n",
    "# 尝试通过网上的方法屏蔽tensorflow的警告\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# 基本配置\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## tensorflow\n",
    "import tensorflow as tf\n",
    "# 一些层和模型的搭建\n",
    "from tensorflow.keras.layers import Dense,Input\n",
    "from tensorflow.keras.layers import Lambda as layer_Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# 自己编写的软更新函数\n",
    "from class_ema import ema_V2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## argparse setting\n",
    "通过该项设置，实现键入**args，外部控制程序的能力，比如想要开始训练，在命令栏输入\n",
    "``` dos\n",
    "python DDPG.py --train\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='Train or test neural net motor controller.')\n",
    "# parser.add_argument('--train', dest='train', action='store_true', default=False)    # 未键入--train时，args.train=Flase\n",
    "# parser.add_argument('--test', dest='test', action='store_true') # 未键入--test时，args.test=Flase\n",
    "# parser.add_argument('--seed',dest='seed',default=3047,type=int) # 未键入--seed 123 时，args.seed=3047\n",
    "# args = parser.parse_args()  # 整合到args内"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 超参数\n",
    "研究表明\n",
    "critic的学习率最好是actor的数倍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME='Pendulum-v1'  # 环境\n",
    "# RANDOMSEED=args.seed    # 随机种子\n",
    "RANDOMSEED=3047   # 随机种子\n",
    "\n",
    "LR_A=0.001              # actor learning rate\n",
    "LR_C=0.002              # critic learning rate\n",
    "GAMMA=0.9               # reward discount rate\n",
    "TAU=0.001               # soft update coe\n",
    "REPLAYBUFFER_SIZE=10000 # size of replay buffer\n",
    "BATCH_SIZE=32           # learn per batch size\n",
    "VAR=3                   # variance of the action for exploration\n",
    "\n",
    "MAX_EPISODES=200        # maximum exploration times(from reset() to done/max_EP_STEP)\n",
    "MAX_EP_STEPS=200        # maximum step per episode\n",
    "TEST_PER_EPISODES=10    # step that test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG(object):\n",
    "    \n",
    "    '''\n",
    "    replay_buffer\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self,s_dim,a_dim,a_bound):\n",
    "        # 先初始化transition (s_t,a_t,r_t,s_{t+1})\n",
    "        self.replay_buffer=np.zeros(REPLAYBUFFER_SIZE,s_dim*2+a_dim*1+1,dtype=np.float32)\n",
    "        self.pointer=0  # 指针\n",
    "        self.s_dim,self.a_dim,self.a_bound=s_dim,a_dim,a_bound\n",
    "\n",
    "        W_init = tf.random_normal_initializer(mean=0,stddev=0.3)\n",
    "        b_init = tf.constant_initializer(0.1)\n",
    "\n",
    "        # 建立四个网络，先定义两个网络架构\n",
    "        def get_actor(input_state_shape,output_shape,name=''):\n",
    "            lb=np.array(output_shape[0])\n",
    "            ub=np.array(output_shape[1])\n",
    "            gap=ub-lb\n",
    "            '''\n",
    "            input:s[n,s_dim]\n",
    "            output:a[n,a_dim] in a_bound\n",
    "\n",
    "            with a struct of\n",
    "            input 64+dense 64+dense 1+tanh 1+(layer_Lambda)\n",
    "\n",
    "            where n is the number of states input\n",
    "            '''\n",
    "            inputs=Input(input_state_shape,name=name+' input')\n",
    "            x=Dense(units=64,activation='relu',kernel_initializer=W_init, bias_initializer=b_init,name=name+' invisiable layer1')(inputs)\n",
    "            x=Dense(units=64,activation='relu',kernel_initializer=W_init, bias_initializer=b_init,name=name+' invisiable layer2')(x)\n",
    "            x=Dense(units=1,activation='sigmoid',kernel_initializer=W_init, bias_initializer=b_init,name=name+' sigmoid layer')(x)\n",
    "            x=layer_Lambda(lambda x:lb+gap*x,name=name+' Lambda layer')\n",
    "            return Model(inputs=inputs,outputs=x,name=name+'network')\n",
    "        \n",
    "        def get_critic(input_s_shape,input_a_shape,name=''):\n",
    "            '''\n",
    "            input :s,a\n",
    "            output:q_value\n",
    "            with a struct of \n",
    "            [input_s,input_a]\n",
    "            '''\n",
    "            input_s=Input(input_s_shape,name=name+' state input')\n",
    "            input_a=Input(input_a_shape,name=name+' action input')\n",
    "            x=tf.concat([input_s,input_a],1)\n",
    "            x=Dense(units=64,activation='relu',kernel_initializer=W_init, bias_initializer=b_init,name=name+' invisiable layer1')(x)\n",
    "            x=Dense(units=64,activation='relu',kernel_initializer=W_init, bias_initializer=b_init,name=name+' invisiable layer2')(x)\n",
    "            x=Dense(units=1,kernel_initializer=W_init, bias_initializer=b_init,name=name+' linear output')(x)\n",
    "            return Model(inputs=[input_s,input_a],outputs=x,name=name+'network')\n",
    "        \n",
    "        # 帮助target网络硬更新\n",
    "        def copy_para(from_model, to_model):\n",
    "            '''\n",
    "            copy the parameter from from_model to to_model\n",
    "            '''\n",
    "            for i,j in zip(from_model.trainable_weights,to_model.trainable_weights):\n",
    "                j.assign(i)\n",
    "        \n",
    "        # 建立四个网络\n",
    "        self.actor          =get_actor(s_dim,output_shape=a_bound,name='actor')\n",
    "        self.actor_target   =get_actor(s_dim,output_shape=a_bound,name='actor_target')\n",
    "        self.critic         =get_critic(s_dim,a_dim,name='critic')\n",
    "        self.critic_target  =get_critic(s_dim,a_dim,name='critic_target')\n",
    "\n",
    "        # 赋值\n",
    "        copy_para(self.actor,self.actor_target)\n",
    "        copy_para(self.critic,self.critic_target)\n",
    "\n",
    "        # 初始化 ExponentialMovingAverage\n",
    "        self.ema=ema_V2(decay=1-TAU)\n",
    "\n",
    "        # 设置优化器\n",
    "        self.actor_opt = tf.optimizers.Adam(LR_A)\n",
    "        self.critic_opt = tf.optimizers.Adam(LR_C)\n",
    "\n",
    "        ############*end* __init__ *end*############\n",
    "    \n",
    "    # 进行ema平滑更新函数\n",
    "    def ema_update(self):\n",
    "        '''\n",
    "        shadow_variable -= (1 - decay) * (shadow_variable - variable)\n",
    "        or shadow_variable = decay * shadow_variable + (1 - decay) * variable(not recommended)\n",
    "        with decay=min(self.decay, (1 + num_updates) / (10 + num_updates))\n",
    "        '''\n",
    "        paras = self.actor.trainable_weights + self.critic.trainable_weights    #获取要更新的参数包括actor和critic的\n",
    "        # self.ema.apply(paras)                                                   #主要是建立影子参数\n",
    "        for i, j in zip(self.actor_target.trainable_weights + self.critic_target.trainable_weights, paras):\n",
    "            self.ema.apply(i)                                                   #主要是建立影子参数\n",
    "            i.assign(self.ema.average(j))  \n",
    "\n",
    "    # 动作选择函数\n",
    "    def choose_action(self,s):\n",
    "        '''\n",
    "        input: state s\n",
    "        output: the action chosen by actor\n",
    "        '''\n",
    "        return self.actor(np.array(s, dtype=np.float32))# 限定输入\n",
    "    \n",
    "    # 存储transition\n",
    "    def store_transition(self,s,a,r,s_):\n",
    "        '''\n",
    "        store transition(s,a,r,s') into the replay buffer\n",
    "        '''\n",
    "        s =s.astype(np.float32)\n",
    "        s_ =s_.astype(np.float32)\n",
    "        \n",
    "        transition=np.hstack((s,a,r,s_))# 横向堆叠这些变量\n",
    "\n",
    "        index = self.pointer % REPLAYBUFFER_SIZE\n",
    "        self.replay_buffer[index,:]=transition\n",
    "        self.pointer+=1\n",
    "\n",
    "    # 学习一轮\n",
    "    def learn(self):\n",
    "        '''\n",
    "        upgrade the parameter using TD_error and Policy Gradient Descent\n",
    "        '''\n",
    "        indices = np.random.choice(REPLAYBUFFER_SIZE,size=BATCH_SIZE) # 先确定一批transition的位置\n",
    "        bt = self.memory[indices, :]                    #根据indices，选取数据bt，相当于随机\n",
    "        bs = bt[:, :self.s_dim]                         #从bt获得数据s\n",
    "        ba = bt[:, self.s_dim:self.s_dim + self.a_dim]  #从bt获得数据a\n",
    "        br = bt[:, -self.s_dim - 1:-self.s_dim]         #从bt获得数据r\n",
    "        bs_ = bt[:, -self.s_dim:]                       #从bt获得数据s'\n",
    "\n",
    "        # 更新critic先\n",
    "        with tf.GradientTape() as tape:         # 通过这种形式获得梯度\n",
    "            a_ = self.actor_target(bs_)         \n",
    "            q_ = self.critic_target([bs_, a_])\n",
    "            y = br + GAMMA * q_\n",
    "            q = self.critic([bs, ba])\n",
    "            td_error = tf.losses.mean_squared_error(y, q) # TD_error MSE(y,q)\n",
    "        c_grads = tape.gradient(td_error, self.critic.trainable_weights) # 得到偏LOSS偏omega\n",
    "        self.critic_opt.apply_gradients(zip(c_grads, self.critic.trainable_weights))# 进行一次梯度下降（目的是降低LOSS）\n",
    "\n",
    "        # 更新actor\n",
    "        with tf.GradientTape() as tape:\n",
    "            a = self.actor(bs)\n",
    "            q = self.critic([bs, a])\n",
    "            a_loss = -tf.reduce_mean(q)  # 【敲黑板】：注意这里用负号，是梯度上升！也就是离目标会越来越远的，就是越来越大。\n",
    "        a_grads = tape.gradient(a_loss, self.actor.trainable_weights) # 得到*负值*偏Q偏theta\n",
    "        self.actor_opt.apply_gradients(zip(a_grads, self.actor.trainable_weights))# 进行一次梯度下降（但是实际是梯度上升）\n",
    "\n",
    "        # 更新一次参数\n",
    "        self.ema_update()\n",
    "\n",
    "    # 保存变量和load变量\n",
    "    def save_ckpt(self):\n",
    "        \"\"\"\n",
    "        save trained weights\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if not os.path.exists('model'):\n",
    "            os.makedirs('model')\n",
    "        else:\n",
    "            now=datetime.datetime.now()\n",
    "            now=now.strftime('%Y-%m-%d-%H%M%S')\n",
    "            new_name='model_'+now\n",
    "            old_name='model'\n",
    "            os.rename(old_name,new_name)\n",
    "            os.makedirs(old_name)\n",
    "            \n",
    "\n",
    "        self.actor.save_weights('model/ddpg_actor.hdf5')\n",
    "        self.actor_target.save_weights('model/ddpg_actor_target.hdf5')\n",
    "        self.critic.save_weights('model/ddpg_critic.hdf5')\n",
    "        self.critic_target.save_weights('model/ddpg_critic_target.hdf5')\n",
    "\n",
    "    def load_ckpt(self):\n",
    "        \"\"\"\n",
    "        load trained weights\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.actor.load_weights('model/ddpg_actor.hdf5')\n",
    "        self.actor_target.load_weights('model/ddpg_actor_target.hdf5')\n",
    "        self.critic.load_weights('model/ddpg_critic.hdf5')\n",
    "        self.critic_target.load_weights('model/ddpg_critic_target.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data type not understood",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m a_bound \u001b[39m=\u001b[39m [a_bound_lb,a_bound_ub]\n\u001b[0;32m     17\u001b[0m \u001b[39m# 初始化DDPG\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m ddpg\u001b[39m=\u001b[39mDDPG(s_dim,a_dim,a_bound)\n\u001b[0;32m     20\u001b[0m \u001b[39m# 开始训练 --train\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m1\u001b[39m:\n\u001b[0;32m     22\u001b[0m     \u001b[39m# 部分信息输出\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [9], line 10\u001b[0m, in \u001b[0;36mDDPG.__init__\u001b[1;34m(self, s_dim, a_dim, a_bound)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m,s_dim,a_dim,a_bound):\n\u001b[0;32m      9\u001b[0m     \u001b[39m# 先初始化transition (s_t,a_t,r_t,s_{t+1})\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplay_buffer\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39;49mzeros(REPLAYBUFFER_SIZE,s_dim\u001b[39m*\u001b[39;49m\u001b[39m2\u001b[39;49m\u001b[39m+\u001b[39;49ma_dim\u001b[39m*\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m,dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat)\n\u001b[0;32m     11\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpointer\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m  \u001b[39m# 指针\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ms_dim,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma_dim,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma_bound\u001b[39m=\u001b[39ms_dim,a_dim,a_bound\n",
      "\u001b[1;31mTypeError\u001b[0m: data type not understood"
     ]
    }
   ],
   "source": [
    "# init\n",
    "env=gym.make(ENV_NAME)\n",
    "env.unwrapped\n",
    "\n",
    "# set seed 方便复现结果\n",
    "env.reset(seed=RANDOMSEED)\n",
    "np.random.seed(RANDOMSEED)\n",
    "tf.random.set_seed(RANDOMSEED)\n",
    "\n",
    "# 得到动作空间参数\n",
    "s_dim = env.observation_space.shape[0]\n",
    "a_dim = env.action_space.shape[0]\n",
    "a_bound_lb = env.action_space.low\n",
    "a_bound_ub = env.action_space.high\n",
    "a_bound = [a_bound_lb,a_bound_ub]\n",
    "\n",
    "# 初始化DDPG\n",
    "ddpg=DDPG(s_dim,a_dim,a_bound)\n",
    "\n",
    "# 开始训练 --train\n",
    "if 1:\n",
    "    # 部分信息输出\n",
    "    print('s_dim',s_dim)\n",
    "    print('a_dim',a_dim)\n",
    "    print('a_bound',a_bound)\n",
    "    print('\\n**********',\n",
    "    f'\\n Random Seed is :\\t {RANDOMSEED}',\n",
    "    '\\n**********')\n",
    "\n",
    "    reward_buffer = []      #用于记录每个EP的reward，统计变化\n",
    "    t0 = time.time()        #统计时间\n",
    "    for i in range(MAX_EPISODES):\n",
    "        t1 = time.time()\n",
    "        s = env.reset()\n",
    "        ep_reward = 0       #记录当前EP的reward\n",
    "        for j in range(MAX_EP_STEPS):\n",
    "            # Add exploration noise\n",
    "            a = ddpg.choose_action(s)       #这里很简单，直接用actor估算出a动作\n",
    "\n",
    "            # 为了能保持开发，这里用了另外一种方式增加探索。\n",
    "            # 因此需要需要以a为均值，VAR为标准差，建立正态分布，再从正态分布采样出a\n",
    "            # 因为a是均值，所以a的概率是最大的。但a相对其他概率由多大，是靠VAR调整。这里我们其实可以增加更新VAR，动态调整a的确定性\n",
    "            # 然后进行裁剪\n",
    "            a = np.clip(np.random.normal(a, VAR), -2, 2)  \n",
    "            # 与环境进行互动\n",
    "            s_, r, done, info = env.step(a)\n",
    "\n",
    "            # 保存s，a，r，s_\n",
    "            ddpg.store_transition(s, a, r / 10, s_)\n",
    "\n",
    "            # 第一次数据满了，就可以开始学习\n",
    "            if ddpg.pointer > REPLAYBUFFER_SIZE:\n",
    "                ddpg.learn()\n",
    "\n",
    "            #输出数据记录\n",
    "            s = s_  \n",
    "            ep_reward += r  #记录当前EP的总reward\n",
    "            if j == MAX_EP_STEPS - 1:\n",
    "                print(\n",
    "                    '\\rEpisode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}'.format(\n",
    "                        i, MAX_EPISODES, ep_reward,\n",
    "                        time.time() - t1\n",
    "                    ), end=''\n",
    "                )\n",
    "            plt.show()\n",
    "        # test\n",
    "        if i and not i % TEST_PER_EPISODES:\n",
    "            t1 = time.time()\n",
    "            s = env.reset()\n",
    "            ep_reward = 0\n",
    "            for j in range(MAX_EP_STEPS):\n",
    "\n",
    "                a = ddpg.choose_action(s)  # 注意，在测试的时候，我们就不需要用正态分布了，直接一个a就可以了。\n",
    "                s_, r, done, info = env.step(a)\n",
    "\n",
    "                s = s_\n",
    "                ep_reward += r\n",
    "                if j == MAX_EP_STEPS - 1:\n",
    "                    print(\n",
    "                        '\\rEpisode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}'.format(\n",
    "                            i, MAX_EPISODES, ep_reward,\n",
    "                            time.time() - t1\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    reward_buffer.append(ep_reward)\n",
    "\n",
    "        if reward_buffer:\n",
    "            plt.ion()\n",
    "            plt.cla()\n",
    "            plt.title('DDPG')\n",
    "            plt.plot(np.array(range(len(reward_buffer))) * TEST_PER_EPISODES, reward_buffer)  # plot the episode vt\n",
    "            plt.xlabel('episode steps')\n",
    "            plt.ylabel('normalized state-action value')\n",
    "            plt.ylim(-2000, 100)\n",
    "            plt.show()\n",
    "            plt.pause(0.1)\n",
    "    print('\\n***end***')\n",
    "    plt.ioff()\n",
    "    # plt.show()      \n",
    "    print('\\nRunning time: ', time.time() - t0)\n",
    "    # time_start_store_weights=time.perf_counter()\n",
    "    # print('store_time')\n",
    "    ddpg.save_ckpt()\n",
    "    plt.savefig('model/result.png')\n",
    "    # print('store time cost is:{:.4f}'.format(time.perf_counter()-time_start_store_weights))\n",
    "    \n",
    "\n",
    "# test\n",
    "if 2:\n",
    "    ddpg.load_ckpt()\n",
    "    while True:\n",
    "        s = env.reset()\n",
    "        for i in range(MAX_EP_STEPS):\n",
    "            env.render()\n",
    "            s, r, done, info = env.step(ddpg.choose_action(s))\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('DDPG_V3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bcb4bec036a3c81b95c978d072a0cec1950f4d5ffbf010d3b190eccffa46df8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
